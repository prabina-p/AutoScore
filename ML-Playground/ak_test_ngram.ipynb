{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prabinapokharel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prabinapokharel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prabinapokharel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import papermill as pm\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus  import stopwords\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Download NLTK resources if not already downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngram_features(data, n):\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), binary=True)\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return X, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Student ID</th>\n",
       "      <th>Email</th>\n",
       "      <th>Status</th>\n",
       "      <th>Submission ID</th>\n",
       "      <th>Total Score</th>\n",
       "      <th>Max Points</th>\n",
       "      <th>Question 1 Score</th>\n",
       "      <th>Question 1 Weight</th>\n",
       "      <th>Question 1 Graded?</th>\n",
       "      <th>Question 1 Response</th>\n",
       "      <th>Question 1 Submitted At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andrew Pan</td>\n",
       "      <td>A10001</td>\n",
       "      <td>andrep24@uw.edu</td>\n",
       "      <td>Ungraded</td>\n",
       "      <td>231798102</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Binary search trees represent a sophisticated ...</td>\n",
       "      <td>2024-02-17 14:04:35 -0800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name Student ID            Email    Status  Submission ID  \\\n",
       "0  Andrew Pan     A10001  andrep24@uw.edu  Ungraded      231798102   \n",
       "\n",
       "   Total Score  Max Points  Question 1 Score  Question 1 Weight  \\\n",
       "0          0.0         1.0               NaN                1.0   \n",
       "\n",
       "   Question 1 Graded?                                Question 1 Response  \\\n",
       "0               False  Binary search trees represent a sophisticated ...   \n",
       "\n",
       "     Question 1 Submitted At  \n",
       "0  2024-02-17 14:04:35 -0800  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/submission_metadata.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Binary search trees represent a sophisticated data structure pivotal for managing and manipulating large datasets with optimal efficiency. Their function transcends simple storage; they offer a systematic arrangement of elements, allowing logarithmic time complexity for search operations through efficient partitioning of the data space. Furthermore, binary search trees facilitate dynamic operations such as insertion and deletion while maintaining their balanced structure, making them indispensable in domains where performance and scalability are paramount, such as database management systems and network routing algorithms.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data = df['Question 1 Response'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>allowing logarithmic time</th>\n",
       "      <th>and deletion while</th>\n",
       "      <th>and manipulating large</th>\n",
       "      <th>and network routing</th>\n",
       "      <th>and scalability are</th>\n",
       "      <th>are paramount such</th>\n",
       "      <th>arrangement of elements</th>\n",
       "      <th>as database management</th>\n",
       "      <th>as insertion and</th>\n",
       "      <th>balanced structure making</th>\n",
       "      <th>...</th>\n",
       "      <th>them indispensable in</th>\n",
       "      <th>they offer systematic</th>\n",
       "      <th>through efficient partitioning</th>\n",
       "      <th>time complexity for</th>\n",
       "      <th>transcends simple storage</th>\n",
       "      <th>trees facilitate dynamic</th>\n",
       "      <th>trees represent sophisticated</th>\n",
       "      <th>where performance and</th>\n",
       "      <th>while maintaining their</th>\n",
       "      <th>with optimal efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   allowing logarithmic time  and deletion while  and manipulating large  \\\n",
       "0                          1                   1                       1   \n",
       "\n",
       "   and network routing  and scalability are  are paramount such  \\\n",
       "0                    1                    1                   1   \n",
       "\n",
       "   arrangement of elements  as database management  as insertion and  \\\n",
       "0                        1                       1                 1   \n",
       "\n",
       "   balanced structure making  ...  them indispensable in  \\\n",
       "0                          1  ...                      1   \n",
       "\n",
       "   they offer systematic  through efficient partitioning  time complexity for  \\\n",
       "0                      1                               1                    1   \n",
       "\n",
       "   transcends simple storage  trees facilitate dynamic  \\\n",
       "0                          1                         1   \n",
       "\n",
       "   trees represent sophisticated  where performance and  \\\n",
       "0                              1                      1   \n",
       "\n",
       "   while maintaining their  with optimal efficiency  \n",
       "0                        1                        1  \n",
       "\n",
       "[1 rows x 76 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, feature_names = generate_ngram_features(text_data, N)\n",
    "ngram_df = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "ngram_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score for n=0 -> 0.024175824175824177\n",
      "Accuracy Score for n=1 -> 0.6879120879120879\n",
      "Accuracy Score for n=2 -> 0.589010989010989\n",
      "Accuracy Score for n=3 -> 0.46813186813186813\n",
      "Accuracy Score for n=4 -> 0.33186813186813185\n",
      "Accuracy Score for n=5 -> 0.23516483516483516\n",
      "Accuracy Score for n=6 -> 0.16923076923076924\n",
      "Accuracy Score for n=7 -> 0.12307692307692308\n",
      "Accuracy Score for n=8 -> 0.1054945054945055\n",
      "Accuracy Score for n=9 -> 0.0945054945054945\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_tfidf_pipeline(data, labels, n):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(n, n), binary=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # print(f'Classification Score for n={n} ->{classification_report(y_test, y_pred)}')\n",
    "    print(f'Accuracy Score for n={n} -> {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def main():\n",
    "\n",
    "    def cleaning(df):\n",
    "        lowered=df.lower() # lowering the sentences \n",
    "        removed = re.sub(r'[^a-z]',' ',lowered)  # replacing the non alphabets with space \n",
    "        splitted=removed.split(' ')   # splitting the sentences by spaces to lemmatize\n",
    "        df = [WordNetLemmatizer().lemmatize(word) for word in splitted \n",
    "            if word not in stopwords.words('english')]  # lemmatizing and removing stopwords\n",
    "        df =' '.join(df) # joining back the words of list\n",
    "        return(removed)\n",
    "\n",
    "    csv_file = 'data/mohler_dataset_edited.csv'\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "    data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "    \n",
    "    text_data = data['student_answer'].tolist()\n",
    "    labels = data['desired_answer'].tolist() \n",
    "    \n",
    "    # n = 3\n",
    "    num_n = 6\n",
    "    for n in range(num_n):\n",
    "        generate_ngram_tfidf_pipeline(text_data, labels, n)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('vectorizer', CountVectorizer(binary=True)),\n                ('tfidf', TfidfTransformer())]) does not.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m         generate_similarity_scores(text_data, labels, n)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m num_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m  \u001b[38;5;66;03m# You can choose your desired n-gram range\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, num_n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mgenerate_similarity_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m, in \u001b[0;36mgenerate_similarity_scores\u001b[0;34m(data, labels, n)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[1;32m     21\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Get best parameters\u001b[39;00m\n\u001b[1;32m     25\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:800\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    798\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 800\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_scoring\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    802\u001b[0m     scorers \u001b[38;5;241m=\u001b[39m _check_multimetric_scoring(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_scorer.py:953\u001b[0m, in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    954\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf no scoring is specified, the estimator passed should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    955\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave a \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method. The estimator \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m estimator\n\u001b[1;32m    956\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator Pipeline(steps=[('vectorizer', CountVectorizer(binary=True)),\n                ('tfidf', TfidfTransformer())]) does not."
     ]
    }
   ],
   "source": [
    "def generate_similarity_scores(data, labels, n):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'vectorizer__ngram_range': [(1, n), (2, n)], \n",
    "        'vectorizer__min_df': [1, 2, 3],\n",
    "        'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(data, labels)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters for n={n}:\", best_params)\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Compute similarity scores\n",
    "    similarity_scores = cosine_similarity(pipeline.transform(X_test), pipeline.transform(y_test))\n",
    "    avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "\n",
    "    print(f'Average Cosine Similarity Score for n={n} -> {avg_similarity_score}')\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def main():\n",
    "    csv_file = 'data/mohler_dataset_edited.csv'\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "    data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "    text_data = data['student_answer'].tolist()\n",
    "    labels = data['desired_answer'].tolist()\n",
    "\n",
    "    num_n = 6  # You can choose your desired n-gram range\n",
    "    for n in range(1, num_n + 1):\n",
    "        generate_similarity_scores(text_data, labels, n)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    # Lowercasing the text\n",
    "    lowered = text.lower() \n",
    "    \n",
    "    # Removing non-alphabetic characters\n",
    "    removed = re.sub(r'[^a-z]', ' ', lowered)  \n",
    "    \n",
    "    # Tokenizing the text\n",
    "    tokens = nltk.word_tokenize(removed)\n",
    "    \n",
    "    # Lemmatizing and removing stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Joining the tokens back into text\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# csv_file = 'data/mohler_dataset_edited.csv'\n",
    "# data = pd.read_csv(csv_file)\n",
    "\n",
    "# # Applying cleaning function to 'desired_answer' and 'student_answer' columns\n",
    "# data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "# data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "# # Printing the cleaned data\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "90 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1365, in fit_transform\n",
      "    self._validate_ngram_range()\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 517, in _validate_ngram_range\n",
      "    raise ValueError(\n",
      "ValueError: Invalid value for ngram_range=(2, 1) lower boundary larger than the upper boundary.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.30083314        nan 0.31052533        nan 0.3149686         nan\n",
      " 0.30083314        nan 0.31052533        nan 0.3149686         nan\n",
      " 0.30083314        nan 0.31052533        nan 0.3149686         nan\n",
      " 0.34599302        nan 0.35064419        nan 0.35342605        nan\n",
      " 0.34599302        nan 0.35064419        nan 0.35342605        nan\n",
      " 0.34599302        nan 0.35064419        nan 0.35342605        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for n=1: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 1)}\n",
      "Average Cosine Similarity Score for n=1 -> 0.3545914727519746\n",
      "Best Parameters for n=2: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 2)}\n",
      "Average Cosine Similarity Score for n=2 -> 0.30732864760267486\n",
      "Best Parameters for n=3: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 3)}\n",
      "Average Cosine Similarity Score for n=3 -> 0.2983127088398887\n",
      "Best Parameters for n=4: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 4)}\n",
      "Average Cosine Similarity Score for n=4 -> 0.295664266106952\n",
      "Best Parameters for n=5: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 5)}\n",
      "Average Cosine Similarity Score for n=5 -> 0.29448990518178186\n",
      "Best Parameters for n=6: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 6)}\n",
      "Average Cosine Similarity Score for n=6 -> 0.29385588084663133\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def cosine_similarity_score(estimator, X, y):\n",
    "    # Compute cosine similarity between transformed test data and labels\n",
    "    similarity_scores = cosine_similarity(estimator.transform(X), estimator.transform(y))\n",
    "    avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "    return avg_similarity_score\n",
    "\n",
    "def generate_similarity_scores(data, labels, n):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "    ])\n",
    "\n",
    "    # Define parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'vectorizer__ngram_range': [(1, n), (2, n)],  # Vary the ngram range\n",
    "        'vectorizer__min_df': [1, 2, 3],\n",
    "        'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "    }\n",
    "\n",
    "    # Perform grid search with custom scoring function\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=cosine_similarity_score)\n",
    "    grid_search.fit(data, labels)\n",
    "\n",
    "    # Get best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters for n={n}:\", best_params)\n",
    "\n",
    "    # Use the best parameters to build the pipeline\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit pipeline on training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Compute similarity scores\n",
    "    similarity_scores = cosine_similarity(pipeline.transform(X_test), pipeline.transform(y_test))\n",
    "    avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "\n",
    "    print(f'Average Cosine Similarity Score for n={n} -> {avg_similarity_score}')\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def main():\n",
    "    csv_file = 'data/mohler_dataset_edited.csv'\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "    data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "    text_data = data['student_answer'].tolist()\n",
    "    labels = data['desired_answer'].tolist()\n",
    "\n",
    "    num_n = 6  # You can choose your desired n-gram range\n",
    "    for n in range(1, num_n + 1):\n",
    "        generate_similarity_scores(text_data, labels, n)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
