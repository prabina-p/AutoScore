{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prabinapokharel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/prabinapokharel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prabinapokharel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus  import stopwords\n",
    "import re\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_ngram_tfidf_pipeline(data, labels, n):\n",
    "#     pipeline = Pipeline([\n",
    "#         ('vectorizer', CountVectorizer(ngram_range=(n, n), binary=True)),\n",
    "#         ('tfidf', TfidfTransformer()),\n",
    "#         ('classifier', RandomForestClassifier())\n",
    "#     ])\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "#     pipeline.fit(X_train, y_train)\n",
    "    \n",
    "#     y_pred = pipeline.predict(X_test)\n",
    "#     # print(f'Classification Score for n={n} ->{classification_report(y_test, y_pred)}')\n",
    "#     print(f'Accuracy Score for n={n} -> {accuracy_score(y_test, y_pred)}')\n",
    "\n",
    "#     return pipeline\n",
    "\n",
    "# def main():\n",
    "\n",
    "#     def cleaning(df):\n",
    "#         lowered=df.lower() # lowering the sentences \n",
    "#         removed = re.sub(r'[^a-z]',' ',lowered)  # replacing the non alphabets with space \n",
    "#         splitted=removed.split(' ')   # splitting the sentences by spaces to lemmatize\n",
    "#         df = [WordNetLemmatizer().lemmatize(word) for word in splitted \n",
    "#             if word not in stopwords.words('english')]  # lemmatizing and removing stopwords\n",
    "#         df =' '.join(df) # joining back the words of list\n",
    "#         return(removed)\n",
    "\n",
    "#     data = pd.read_csv('data/mohler_dataset_edited.csv')\n",
    "#     data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "#     data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "#     text_data = data['student_answer'].tolist()\n",
    "#     labels = data['desired_answer'].tolist() \n",
    "    \n",
    "#     num_n = 6\n",
    "#     for n in range(num_n):\n",
    "#         generate_ngram_tfidf_pipeline(text_data, labels, n)\n",
    "        \n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cleaning(text):\n",
    "#     lowered = text.lower() \n",
    "    \n",
    "#     removed = re.sub(r'[^a-z]', ' ', lowered)  \n",
    "#     tokens = nltk.word_tokenize(removed)\n",
    "    \n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "#     cleaned_text = ' '.join(cleaned_tokens)\n",
    "    \n",
    "#     return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "90 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 416, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 370, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/joblib/memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 950, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1365, in fit_transform\n",
      "    self._validate_ngram_range()\n",
      "  File \"/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 517, in _validate_ngram_range\n",
      "    raise ValueError(\n",
      "ValueError: Invalid value for ngram_range=(2, 1) lower boundary larger than the upper boundary.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/prabinapokharel/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.30083314        nan 0.31052533        nan 0.3149686         nan\n",
      " 0.30083314        nan 0.31052533        nan 0.3149686         nan\n",
      " 0.30083314        nan 0.31052533        nan 0.3149686         nan\n",
      " 0.34599302        nan 0.35064419        nan 0.35342605        nan\n",
      " 0.34599302        nan 0.35064419        nan 0.35342605        nan\n",
      " 0.34599302        nan 0.35064419        nan 0.35342605        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for n=1: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 1)}\n",
      "Average Cosine Similarity Score for n=1 -> 0.3545914727519746\n",
      "Best Parameters for n=2: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 2)}\n",
      "Average Cosine Similarity Score for n=2 -> 0.30732864760267486\n",
      "Best Parameters for n=3: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 3)}\n",
      "Average Cosine Similarity Score for n=3 -> 0.2983127088398887\n",
      "Best Parameters for n=4: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 4)}\n",
      "Average Cosine Similarity Score for n=4 -> 0.295664266106952\n",
      "Best Parameters for n=5: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 5)}\n",
      "Average Cosine Similarity Score for n=5 -> 0.29448990518178186\n",
      "Best Parameters for n=6: {'tfidf__use_idf': False, 'vectorizer__max_df': 0.5, 'vectorizer__min_df': 3, 'vectorizer__ngram_range': (1, 6)}\n",
      "Average Cosine Similarity Score for n=6 -> 0.29385588084663133\n"
     ]
    }
   ],
   "source": [
    "# def cosine_similarity_score(estimator, X, y):\n",
    "#     similarity_scores = cosine_similarity(estimator.transform(X), estimator.transform(y))\n",
    "#     avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "#     return avg_similarity_score\n",
    "\n",
    "# def generate_similarity_scores(data, labels, n):\n",
    "#     pipeline = Pipeline([\n",
    "#         ('vectorizer', CountVectorizer(binary=True)),\n",
    "#         ('tfidf', TfidfTransformer()),\n",
    "#     ])\n",
    "\n",
    "#     param_grid = {\n",
    "#         'vectorizer__ngram_range': [(1, n), (2, n)],\n",
    "#         'vectorizer__min_df': [1, 2, 3],\n",
    "#         'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "#         'tfidf__use_idf': [True, False],\n",
    "#     }\n",
    "\n",
    "#     grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=cosine_similarity_score)\n",
    "#     grid_search.fit(data, labels)\n",
    "\n",
    "#     best_params = grid_search.best_params_\n",
    "#     print(f\"Best Parameters for n={n}:\", best_params)\n",
    "\n",
    "#     pipeline.set_params(**best_params)\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#     pipeline.fit(X_train, y_train)\n",
    "\n",
    "#     similarity_scores = cosine_similarity(pipeline.transform(X_test), pipeline.transform(y_test))\n",
    "#     avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "\n",
    "#     print(f'Average Cosine Similarity Score for n={n} -> {avg_similarity_score}')\n",
    "\n",
    "#     return pipeline\n",
    "\n",
    "# def main():\n",
    "#     csv_file = 'data/mohler_dataset_edited.csv'\n",
    "#     data = pd.read_csv(csv_file)\n",
    "#     data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "#     data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "#     text_data = data['student_answer'].tolist()\n",
    "#     labels = data['desired_answer'].tolist()\n",
    "\n",
    "#     num_n = 6\n",
    "#     for n in range(1, num_n + 1):\n",
    "#         generate_similarity_scores(text_data, labels, n)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>student_answer</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>High risk problems are address in the prototyp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>To simulate portions of the desired final prod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>A prototype program simulates the behaviors of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>Defined in the Specification phase a prototype...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the role of a prototype program in pro...</td>\n",
       "      <td>It is used to let the users have a first idea ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2292</th>\n",
       "      <td>What is a queue?</td>\n",
       "      <td>A first in first out data structure</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2293</th>\n",
       "      <td>What is a queue?</td>\n",
       "      <td>A queue is a stack of sequenced tasks, underta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2294</th>\n",
       "      <td>What is a queue?</td>\n",
       "      <td>A queue in computer science is the 81st or 113...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2295</th>\n",
       "      <td>What is a queue?</td>\n",
       "      <td>a queue is a abstract data type with a private...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2296</th>\n",
       "      <td>What is a queue?</td>\n",
       "      <td>A data structure that can store elements, whic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2297 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     What is the role of a prototype program in pro...   \n",
       "1     What is the role of a prototype program in pro...   \n",
       "2     What is the role of a prototype program in pro...   \n",
       "3     What is the role of a prototype program in pro...   \n",
       "4     What is the role of a prototype program in pro...   \n",
       "...                                                 ...   \n",
       "2292                                   What is a queue?   \n",
       "2293                                   What is a queue?   \n",
       "2294                                   What is a queue?   \n",
       "2295                                   What is a queue?   \n",
       "2296                                   What is a queue?   \n",
       "\n",
       "                                         student_answer  correct  \n",
       "0     High risk problems are address in the prototyp...        0  \n",
       "1     To simulate portions of the desired final prod...        0  \n",
       "2     A prototype program simulates the behaviors of...        0  \n",
       "3     Defined in the Specification phase a prototype...        0  \n",
       "4     It is used to let the users have a first idea ...        0  \n",
       "...                                                 ...      ...  \n",
       "2292               A first in first out data structure         1  \n",
       "2293  A queue is a stack of sequenced tasks, underta...        0  \n",
       "2294  A queue in computer science is the 81st or 113...        0  \n",
       "2295  a queue is a abstract data type with a private...        1  \n",
       "2296  A data structure that can store elements, whic...        1  \n",
       "\n",
       "[2297 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/data_queue_final.csv\", index_col=False)\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_OG = df['student_answer'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2240"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_responses_OG = [df.loc[i, 'student_answer'] for i in df.index if df.loc[i, 'correct'] == 1]\n",
    "incorrect_responses_OG = [df.loc[i, 'student_answer'] for i in df.index if df.loc[i, 'correct'] == 0]\n",
    "len(incorrect_responses_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_question = \"What is a queue?\"\n",
    "query_response_OG = \"A queue is a data structure that follows FIFO principle, meaning that the first element added to the queue will be the first one to be removed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    lowered = text.lower() \n",
    "    removed = re.sub(r'[^a-z]', ' ', lowered)  \n",
    "    tokens = nltk.word_tokenize(removed)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['student_answer'] = df['student_answer'].apply(cleaning)\n",
    "\n",
    "responses = df['student_answer'].tolist()\n",
    "\n",
    "correct_responses = [df.loc[i, 'student_answer'] for i in df.index if df.loc[i, 'correct'] == 1]\n",
    "incorrect_responses = [df.loc[i, 'student_answer'] for i in df.index if df.loc[i, 'correct'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_response = cleaning(query_response_OG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9891304347826086\n"
     ]
    }
   ],
   "source": [
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', CountVectorizer(binary=True)),\n",
    "#     ('tfidf', TfidfTransformer()),\n",
    "# ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['student_answer'], df['correct'], test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "    ('classifier', SVC())  # Example classifier, you can replace it with any other classifier\n",
    "])\n",
    "\n",
    "# Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    lowered = text.lower() \n",
    "    removed = re.sub(r'[^a-z]', ' ', lowered)  \n",
    "    tokens = nltk.word_tokenize(removed)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stopwords.words('english')]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def avg_cosine_similarity_score(estimator, X, y):\n",
    "    similarity_scores = cosine_similarity(estimator.transform(X), estimator.transform(y))\n",
    "    avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "    return avg_similarity_score\n",
    "\n",
    "def generate_similarity_scores(data, labels, n):\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(binary=True)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'vectorizer__ngram_range': [(1, n), (2, n)],\n",
    "        'vectorizer__min_df': [1, 2, 3],\n",
    "        'vectorizer__max_df': [0.5, 0.75, 1.0],\n",
    "        'tfidf__use_idf': [True, False],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=avg_cosine_similarity_score)\n",
    "    grid_search.fit(data, labels)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters for n={n}:\", best_params)\n",
    "\n",
    "    pipeline.set_params(**best_params)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    similarity_scores = cosine_similarity(pipeline.transform(X_test), pipeline.transform(y_test))\n",
    "    avg_similarity_score = np.mean(similarity_scores.diagonal())\n",
    "\n",
    "    print(f'Average Cosine Similarity Score for n={n} -> {avg_similarity_score}')\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "def main():\n",
    "    csv_file = 'data/mohler_dataset_edited.csv'\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data['desired_answer'] = data['desired_answer'].apply(cleaning)\n",
    "    data['student_answer'] = data['student_answer'].apply(cleaning)\n",
    "\n",
    "    text_data = data['student_answer'].tolist()\n",
    "    labels = data['desired_answer'].tolist()\n",
    "\n",
    "    num_n = 6\n",
    "    for n in range(1, num_n + 1):\n",
    "        generate_similarity_scores(text_data, labels, n)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
